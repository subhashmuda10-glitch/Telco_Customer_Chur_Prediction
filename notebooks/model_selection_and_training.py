# -*- coding: utf-8 -*-
"""Model_selection_and_training.ipynd

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N0q0kJ0S5Ml2ybiSqQI6Nu4kpGFm1ksK

### **Model selection and Training**

In this notebook, we train multiple machine learning models to predict customer churn.
We compare their performance using accuracy, precision, recall, F1-score, ROC-AUC, and confusion matrix.
We also perform hyperparameter tuning to improve the model performance and choose the best algorithm.

**Models used:**

Logistic Regression

K-Nearest Neighbors (KNN)

Random Forest

XGBoost

CatBoost

Tuned versions of all applicable models

The goal is to identify the best performing model for churn prediction.

## Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""## Evaluating Model Function

**Unified Evaluation Function**

To keep the model comparison consistent, we define a single evaluation function that calculates:

Confusion Matrix

Precision, Recall, F1-Score

Overall Accuracy

ROC-AUC (if supported)

This avoids repeating the same code for every model.
"""

# ---------------------
# 1. Import Libraries
# ---------------------
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
# ... other imports

# ---------------------
# 2. Define Evaluation Function
# ---------------------
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)

    acc=accuracy_score(y_test, y_pred)
    print("Accuracy Score:", acc)
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("\nClassification Report:\n", classification_report(y_test, y_pred))


    try:
        y_prob = model.predict_proba(X_test)[:, 1]
        print("ROC-AUC Score:", roc_auc_score(y_test, y_prob))
    except:
        print("ROC-AUC not available (model has no predict_proba)")
    return acc

"""## Load the train and test Data"""

train = pd.read_csv("train_final.csv")
test = pd.read_csv("test_final.csv")

X_train = train.drop("target", axis=1).values
y_train = train["target"].values

X_test = test.drop("target", axis=1).values
y_test = test["target"].values

print(X_train)
print(y_train)

"""## Logistic Regression

Logistic Regression is a simple and interpretable classification model.
It works well as a baseline and helps us understand feature relationships.

We train the model on scaled numerical + encoded categorical data and evaluate performance.
"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=500, solver='lbfgs')
lr.fit(X_train, y_train)

lr_acc=evaluate_model(lr, X_test, y_test)

"""## K- Nearest Neighbour

KNN is a distance-based algorithm.
It predicts class labels based on the majority vote of nearest neighbors.

It is sensitive to:

Feature scaling

High dimensionality

Choice of k value
"""

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)
knn.fit(X_train,y_train)

knn_acc = evaluate_model(knn, X_test, y_test)

"""## Random Forest

Random Forest is an ensemble of decision trees trained using bagging.
It handles nonlinear relationships and works well on structured data.

Advantages:

Robust to noise

Handles categorical encoding

Provides feature importance

We evaluate both default and tuned versions.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

rf = RandomForestClassifier(n_estimators=500, random_state=42)
rf.fit(X_train, y_train)

rf_acc = evaluate_model(rf, X_test, y_test)

"""### Random Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV

param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf_grid = RandomizedSearchCV(
    RandomForestClassifier(),
    param_distributions=param_grid,
    n_iter=10,
    cv=3,
    verbose=2,
    n_jobs=-1
)

rf_grid.fit(X_train, y_train)
print("Best Params:", rf_grid.best_params_)

best_rf = RandomForestClassifier(**rf_grid.best_params_)
best_rf.fit(X_train, y_train)

print("\n--- Tuned Random Forest Performance ---")
best_rf_acc = evaluate_model(best_rf, X_test, y_test)

"""## XG BOOST

XGBoost is a gradient boosting algorithm optimized for speed and performance.
It performs exceptionally well for tabular data and handles class imbalance effectively.

We train both:

Base model

Tuned model using RandomizedSearchCV
"""

from xgboost import XGBClassifier

xgb = XGBClassifier(
    learning_rate=0.05,
    max_depth=5,
    n_estimators=400,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric='logloss'
)

xgb.fit(X_train, y_train)

xgb_acc = evaluate_model(xgb, X_test, y_test)

"""### XGBoost Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBClassifier

# Parameter grid for XGBoost
xgb_param_grid = {
    'n_estimators': [200, 300, 500, 700],
    'learning_rate': [0.01, 0.03, 0.05, 0.1],
    'max_depth': [3, 5, 7, 10],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 1, 5],
    'reg_lambda': [1, 5, 10]
}

xgb_model = XGBClassifier(
    eval_metric='logloss',
    objective='binary:logistic',
    use_label_encoder=False,
    nthread=-1
)

xgb_random = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=xgb_param_grid,
    n_iter=20,
    scoring='accuracy',
    cv=3,
    verbose=2,
    n_jobs=-1,
    random_state=42
)

# Fit on training data
xgb_random.fit(X_train, y_train)

print("\nBest XGBoost Parameters:", xgb_random.best_params_)

best_xgb = XGBClassifier(
    **xgb_random.best_params_,
    eval_metric='logloss',
    use_label_encoder=False
)

best_xgb.fit(X_train, y_train)

print("\n--- Tuned XGBoost Performance ---")
best_xgb_acc = evaluate_model(best_xgb, X_test, y_test)

"""## CatBoost

CatBoost is a gradient boosting model that performs particularly well with categorical features.
It uses ordered boosting and prevents overfitting.

It generally performs strongly even without heavy tuning.
"""

!pip install catboost

from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

cat = CatBoostClassifier(
    iterations=500,
    learning_rate=0.05,
    depth=6,
    loss_function='Logloss',
    eval_metric='Accuracy',
    verbose=0
)

cat.fit(X_train, y_train)

cat_acc = evaluate_model(cat, X_test, y_test)

"""### CatBoost Hyperparameter Tuning"""

from catboost import CatBoostClassifier

cat_model = CatBoostClassifier(
    silent=True,
    loss_function='Logloss',
    eval_metric='Accuracy'
)

cat_param_grid = {
    'iterations': [300, 500, 800, 1000],
    'learning_rate': [0.01, 0.03, 0.05],
    'depth': [4, 6, 8, 10],
    'l2_leaf_reg': [1, 3, 5, 7, 9],
    'bagging_temperature': [0.1, 0.3, 0.5, 1],
    'border_count': [32, 64, 128]
}

cat_random = RandomizedSearchCV(
    estimator=cat_model,
    param_distributions=cat_param_grid,
    n_iter=20,
    scoring='accuracy',
    cv=3,
    verbose=2,
    n_jobs=-1,
    random_state=42
)

# Fit on training data
cat_random.fit(X_train, y_train)

print("\nBest CatBoost Parameters:", cat_random.best_params_)

best_cat = CatBoostClassifier(
    **cat_random.best_params_,
    silent=True,
    loss_function='Logloss',
    eval_metric='Accuracy'
)

best_cat.fit(X_train, y_train)

print("\n--- Tuned CatBoost Performance ---")
best_cat_acc = evaluate_model(best_cat, X_test, y_test)

"""# Model Comparison

We compare all models using a single table of metrics to identify the best-performing classifier.

Metrics included:

Accuracy

Precision

Recall

F1-Score

ROC-AUC

This helps us finalize the winning model for deployment.
"""

import pandas as pd

results = {
    "Model": ["Logistic Regression", "KNN", "Random Forest", "Random Forest (Tuned)",
              "XGBoost", "XGBoost (Tuned)", "CatBoost", "CatBoost (Tuned)"],

    "Accuracy": [
        lr_acc, knn_acc, rf_acc, best_rf_acc,
        xgb_acc, best_xgb_acc, cat_acc, best_cat_acc
    ]
}

df_results = pd.DataFrame(results)
df_results

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12,6))
sns.barplot(x=df_results['Model'], y=df_results['Accuracy'], palette='viridis')
plt.xticks(rotation=45, ha='right')
plt.ylabel("Accuracy Score")
plt.title("Model Performance Comparison")
plt.show()

best_model_name = df_results.loc[df_results['Accuracy'].idxmax(), 'Model']
best_model_acc = df_results['Accuracy'].max()

print("Best Model:", best_model_name)
print("Best Accuracy:", best_model_acc)

# Store all models in a dictionary
models = {
    "Logistic Regression": lr,
    "KNN": knn,
    "Random Forest": rf,
    "Random Forest (Tuned)": best_rf,
    "XGBoost": xgb,
    "XGBoost (Tuned)": best_xgb,
    "CatBoost": cat,
    "CatBoost (Tuned)": best_cat
}

# Find best model name from results table
best_model_name = df_results.loc[df_results['Accuracy'].idxmax(), 'Model']
best_model = models[best_model_name]   # ‚Üê THIS IS THE REAL MODEL

print("Best Model:", best_model_name)

# Save the best model
import pickle
with open("best_model.pkl", "wb") as f:
    pickle.dump(best_model, f)